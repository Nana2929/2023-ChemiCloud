{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2023 ChemiCloud Task 3 \n",
    "關鍵字元組的抽取\n",
    "是否能夠看成兩個子任務 NER + RE (Relation Extraction) \n",
    "，以現有checkpoints來完成？\n",
    "- [ckip tagger ner paper](https://arxiv.org/pdf/1908.11046.pdf) (Att BiLSTM CNN 的數據)\n",
    "- [Ckip 使用 OntoNotes Entity type list](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf)\n",
    "\n",
    "Entity|Description\n",
    "|---|---|\n",
    "PERSON|People, including fictional\n",
    "NORP | Nationalities or religious or political groups\n",
    "FACILITY| Buildings, airports, highways, bridges, etc.\n",
    "ORGANIZATION | Companies, agencies, institutions, etc.\n",
    "GPE | Countries, cities, states\n",
    "LOCATION | Non-GPE locations, mountain ranges, bodies of water\n",
    "PRODUCT | Vehicles, weapons, foods, etc. (Not services)\n",
    "EVENT | Named hurricanes, battles, wars, sports events, etc.\n",
    "WORK OF ART| Titles of books, songs, etc.\n",
    "LAW | Named documents made into laws \n",
    "LANGUAGE| Any named language\n",
    "DATE | Absolute or relative dates or periods\n",
    "TIME | Times smaller than a day\n",
    "PERCENT| Percentage (including “%”)\n",
    "MONEY | Monetary values, including unit\n",
    "QUANTITY| Measurements, as of weight or distance\n",
    "ORDINAL| “first”, “second”\n",
    "CARDINAL| Numerals that do not fall under another type\n",
    "\n",
    "- ckip tagger \n",
    "- link: https://github.com/ckiplab/ckiptagger \n",
    "\n",
    "|F1 score| OntoNotes 5.0 |WNUT \n",
    "|---|---|---|\n",
    "|ckip tagger NER |88.4%+-0.18|42.26%+-0.82\n",
    "\n",
    "- ckip transformers \n",
    "- link: https://github.com/ckiplab/ckip-transformers \n",
    "\n",
    "|F1 score| OntoNotes 5.0 |\n",
    "|---|---|\n",
    "|ckip transformers NER | 81.17% # bert-base-chinese \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "root = '/home/nanaeilish/projects/2023-chemicloud/'\n",
    "data_dir = os.path.join(root, 'data')\n",
    "filename = '食品安全_ws.csvpkl'\n",
    "df = pd.read_pickle(os.path.join(data_dir, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlecol = '新聞標題'\n",
    "textcol = '新聞內容' \n",
    "texts = df[textcol].tolist()\n",
    "titles = df[titlecol].tolist() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ckip Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 133/133 [00:00<00:00, 996.80it/s]\n",
      "Inference: 100%|██████████| 2/2 [01:14<00:00, 37.39s/it]\n",
      "Tokenization: 100%|██████████| 133/133 [00:00<00:00, 84139.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from ckip_transformers.nlp import CkipPosTagger, CkipNerChunker \n",
    "\n",
    "ner_driver = CkipNerChunker(model=\"bert-base\") \n",
    "\n",
    "df['ner_ckiptrans_text'] = ner_driver(texts) \n",
    "df['ner_ckiptrans_title'] = ner_driver(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored \n",
    "from typing import List, Dict\n",
    "from ckip_transformers.nlp.util import NerToken \n",
    "\n",
    "\n",
    "colormap = {\n",
    "    'PERSON': 'red',\n",
    "    'FOOD': 'blue',\n",
    "    'PRODUCT': 'blue',\n",
    "    'CHEMICAL': 'magenta',      \n",
    "    'ORG': 'green',\n",
    "    'DATE': 'yellow', \n",
    "}\n",
    "\n",
    "\n",
    "def color_ner_text(text: str, \n",
    "                   nertokens: List[NerToken], \n",
    "                   colormap: Dict = colormap,)-> None:\n",
    "    \"\"\" Color the text with the NER tokens. \n",
    "        Args:\n",
    "            text (str): The text to be colored.\n",
    "            nertokens (List[NerToken]): The NER tokens.\n",
    "                It can be simulated with an object with the following attributes:\n",
    "                    ner (str): The NER tag.\n",
    "                    idx (Tuple[int, int]): The start and end index of the token. \n",
    "    \"\"\"\n",
    "    nertokens = sorted(nertokens, key=lambda x: x.idx[0], reverse = True)\n",
    "    for nertoken in nertokens:\n",
    "        b, e = nertoken.idx \n",
    "        color = colormap.get(nertoken.ner, 'cyan')\n",
    "        typed_text = text[b:e] + f'_{nertoken.ner}' + ' '\n",
    "        span = colored(typed_text, color) \n",
    "        text = text[:b] + span + text[e:]\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'快訊 哈根達斯 香草冰淇淋 再驗出禁用農藥 香港喊停售'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_id = 13 \n",
    "df.iloc[news_id][titlecol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== ckip transformers ===\n",
      "▲\u001b[31m哈根達斯_PERSON \u001b[0m香草冰淇淋再被驗出致癌物環氧乙烷。（圖／食藥署提供）\n",
      "記者\u001b[31m鄒鎮宇_PERSON \u001b[0m／綜合報導\n",
      "\u001b[36m美國_GPE \u001b[0m知名冰淇淋品牌「Häagen-Dazs（哈根達斯）」日前進口\u001b[36m台灣_GPE \u001b[0m的「香草冰淇淋」驗出禁用農藥，被邊境攔下依規定銷毀。沒想到，\u001b[32m香港食物環境衞生署_ORG \u001b[0m食物安全中心\u001b[33m10日_DATE \u001b[0m發出公告，\u001b[31m哈根達斯_PERSON \u001b[0m的香草冰淇淋被驗出\u001b[32m歐盟_ORG \u001b[0m禁用的農藥環氧乙烷，因此要求業界停止使用或出售。\n",
      "據《東網》報導，\u001b[32m香港食物環境衞生署_ORG \u001b[0m食物安全中心發現，哈根達斯\u001b[36m75毫升_QUANTITY \u001b[0m、\u001b[36m100毫升_QUANTITY \u001b[0m、\u001b[36m473毫升_QUANTITY \u001b[0m、\u001b[36m9.46公升_QUANTITY \u001b[0m裝的香草冰淇淋被驗出農藥環氧乙烷，因此立刻跟進口商溝通，並通知業界停止使用或出售，後續將進行調查。\n",
      "\u001b[31m哈根達斯_PERSON \u001b[0m\u001b[33m6月21日_DATE \u001b[0m時也在\u001b[36m香港_GPE \u001b[0m被驗出有環氧乙烷，當時\u001b[36m香港_GPE \u001b[0m\u001b[31m哈根達斯_PERSON \u001b[0m致歉，並停售、撤回商品，豈料\u001b[33m本月10日_DATE \u001b[0m又再被驗出含有環氧乙烷。\n",
      "據悉，\u001b[31m哈根達斯_PERSON \u001b[0m\u001b[33m6月21日_DATE \u001b[0m時進口\u001b[36m台灣_GPE \u001b[0m的香草冰淇淋也驗出環氧乙烷，在邊境被攔下1164盒、\u001b[36m5471.34公斤_QUANTITY \u001b[0m的產品，因不符合食品安全衛生管理法\u001b[36m第15_ORDINAL \u001b[0m條有關「農藥殘留容許量標準」規定，須依規定退運或銷毀。\n",
      "\u001b[32m食藥署_ORG \u001b[0m北區管理中心簡任技正\u001b[31m吳宗熹_PERSON \u001b[0m過去受訪時指出，環氧乙烷為農藥一種，具致癌風險，依目前規定不在食品中檢出，國內也沒有核准作為農藥使用，因此進口產品也不得檢出。\n",
      "►按這訂閱Podcast《\u001b[36m小編沒收工_WORK_OF_ART \u001b[0m》每天熱門話題聊不完\n"
     ]
    }
   ],
   "source": [
    "print('==== ckip transformers ===')\n",
    "text = df.iloc[news_id][textcol] \n",
    "nertokens = df.iloc[news_id]['ner_text']\n",
    "color_ner_text(text = text, nertokens = nertokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ckip Tagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1, 10, 'PRODUCT', '哈根達斯香草冰淇淋')\n",
    "from typing import Tuple \n",
    "from easydict import EasyDict as edict \n",
    "\n",
    "def quad2nertoken(quad: Tuple):\n",
    "    \"\"\" Convert a quad to a NER token.\n",
    "    \n",
    "        Args:\n",
    "            quad (Tuple): The quad to be converted.\n",
    "                It should be (start, end, ner, text).\n",
    "        Returns:\n",
    "            A NER token-like object. Simulated with edict() \n",
    "    \"\"\"\n",
    "    nertok = edict()\n",
    "    nertok.idx = (quad[0], quad[1])\n",
    "    nertok.ner = quad[2]\n",
    "    nertok.text = quad[3]\n",
    "    return nertok \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1efHsY16pxK0lBD2gYCgCTnv1Swstq771\n",
      "To: /home/nanaeilish/projects/2023-chemicloud/ckiptagger/data.zip\n",
      "100%|██████████| 1.88G/1.88G [00:37<00:00, 49.6MB/s]\n",
      "/home/nanaeilish/micromamba/envs/chemicloud/lib/python3.8/site-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "2023-02-26 14:36:48.613837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-26 14:36:48.614050: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-26 14:36:48.614078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-26 14:36:48.614097: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-26 14:36:48.614114: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-26 14:36:48.614126: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-02-26 14:36:49.265169: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-26 14:36:49.283076: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "/home/nanaeilish/micromamba/envs/chemicloud/lib/python3.8/site-packages/ckiptagger/model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "/home/nanaeilish/micromamba/envs/chemicloud/lib/python3.8/site-packages/ckiptagger/model_ner.py:57: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    }
   ],
   "source": [
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "ckiptagger_path = \"./ckiptagger\"\n",
    "data_utils.download_data_gdown(ckiptagger_path) # gdrive-ckip\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "\n",
    "ckipdata_path = f'{ckiptagger_path}/data'\n",
    "ws = WS(ckipdata_path , disable_cuda=False)\n",
    "pos = POS(ckipdata_path, disable_cuda=False)\n",
    "ner = NER(ckipdata_path, disable_cuda=False)\n",
    "\n",
    "\n",
    "# titles \n",
    "\n",
    "word_sentence_list = ws(\n",
    "    titles,\n",
    "    # sentence_segmentation = True, # To consider delimiters\n",
    "    # segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}), # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "pos_sentence_list = pos(word_sentence_list)\n",
    "entity_sentence_list = ner(word_sentence_list, pos_sentence_list) \n",
    "df['ner_ckiptagger_title'] = [list(map(quad2nertoken, entity)) for entity in entity_sentence_list]\n",
    "\n",
    "# texts \n",
    "word_sentence_list = ws(\n",
    "    texts,\n",
    "    # sentence_segmentation = True, # To consider delimiters\n",
    "    # segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}), # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")\n",
    "pos_sentence_list = pos(word_sentence_list)\n",
    "entity_sentence_list = ner(word_sentence_list, pos_sentence_list) \n",
    "\n",
    "df['ner_ckiptagger_text'] = [list(map(quad2nertoken, entity)) for entity in entity_sentence_list] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== ckip tagger ===\n",
      "1\n",
      "▲\u001b[36m日本_GPE \u001b[0m\u001b[36m福島_GPE \u001b[0m核災\u001b[36m五_CARDINAL \u001b[0m縣市食品有條件輸入\u001b[36m台灣_GPE \u001b[0m。（示意圖／記者\u001b[31m林敬旻_PERSON \u001b[0m攝）\n",
      "\n",
      "文／食力foodNEXT\n",
      "採訪／李依文\n",
      "撰文／\u001b[31m黃宜稜_PERSON \u001b[0m、\u001b[36m李依文_NORP \u001b[0m\n",
      "\u001b[33m2011年3月11日_DATE \u001b[0m\u001b[36m日本_GPE \u001b[0m東部發生地震規模\u001b[36m9.0_CARDINAL \u001b[0m的大地震，為\u001b[36m日本_GPE \u001b[0m有紀錄以來規模最大的地震，並伴隨著海嘯與餘震引發一連串大規模的災害，其中\u001b[36m福島第一核電廠事故_EVENT \u001b[0m更因此造成設備損害、爐心熔毀、輻射釋放等災害，是為自\u001b[33m1986年_DATE \u001b[0m\u001b[36m車諾比核電廠事件_EVENT \u001b[0m後最嚴重的核災事件。由於輻射外洩恐污染臨近區域所種植、製造的食品，因此各國政府與\u001b[36m日本_NORP \u001b[0m官方開始制定一連串的政策與檢驗標準。\n",
      "\u001b[33m2011年3月_DATE \u001b[0m\u001b[36m福島_GPE \u001b[0m\u001b[36m5_CARDINAL \u001b[0m縣食品暫停輸台\u001b[36m 9_CARDINAL \u001b[0m大類每日逐批查驗\n",
      "\u001b[36m台灣_GPE \u001b[0m自\u001b[33m2011年3月25日_DATE \u001b[0m暫停受理\u001b[36m日本_GPE \u001b[0m福島縣與鄰近的\u001b[36m茨城縣_GPE \u001b[0m、\u001b[36m櫪木縣_GPE \u001b[0m、\u001b[36m群馬縣_GPE \u001b[0m、\u001b[36m千葉縣_GPE \u001b[0m等共\u001b[36m5_CARDINAL \u001b[0m個縣市生產製造的食品輸入，而對於\u001b[36m日本_GPE \u001b[0m\u001b[36m福島_GPE \u001b[0m\u001b[36m5_CARDINAL \u001b[0m縣以外地區的食品輸入\u001b[36m台灣_GPE \u001b[0m，則針對\u001b[36m9_CARDINAL \u001b[0m大類食品包含生鮮冷藏蔬果、冷凍蔬果、活生鮮冷藏水產品、冷凍水產品、乳製品、嬰幼兒食品、礦泉水或飲水、海草類、茶葉等實施逐批查驗，並送請\u001b[32m行政院_ORG \u001b[0m原子能委員會檢測其輻射量，以加馬能譜分析碘-\u001b[36m13_CARDINAL \u001b[0m1、銫-134、\u001b[34m銫-137_PRODUCT \u001b[0m等人工核種，每個工作天於\u001b[32m食藥署_ORG \u001b[0m「\u001b[32m日本食品管理工作專區_ORG \u001b[0m」公告結果，截自\u001b[33m2016年11月23日_DATE \u001b[0m累積檢驗數已達\u001b[36m9萬4438_CARDINAL \u001b[0m件，其中雖有\u001b[36m216_CARDINAL \u001b[0m件被檢出微量輻射，但檢出值並未超過我國與\u001b[36m日本_GPE \u001b[0m所制定的「原子塵或放射能污染安全容許量標準」。\n",
      "\u001b[33m2013年7月_DATE \u001b[0m輻射水再度外洩 \u001b[36m福島_GPE \u001b[0m\u001b[36m5_CARDINAL \u001b[0m縣持續暫停輸台\n",
      "\u001b[33m2013年7月初_DATE \u001b[0m，\u001b[36m福島_GPE \u001b[0m第一核電廠再度發生輻射水外洩事件，而\u001b[32m日本東京電力公司_ORG \u001b[0m卻直到\u001b[33m8月31日_DATE \u001b[0m才出面證實，因此\u001b[33m同年9月6日_DATE \u001b[0m\u001b[36m韓國_GPE \u001b[0m宣布全面禁止自\u001b[36m福島_GPE \u001b[0m及周圍共\u001b[36m8_CARDINAL \u001b[0m縣市的水產品進口，而\u001b[36m台灣_GPE \u001b[0m除了持續暫停受理\u001b[36m福島_GPE \u001b[0m\u001b[36m5_CARDINAL \u001b[0m縣的食品輸入\u001b[36m台灣_GPE \u001b[0m之外，也在\u001b[33m同年11月27日_DATE \u001b[0m由\u001b[32m立法院_ORG \u001b[0m全體委員會議決議「\u001b[32m衛福部_ORG \u001b[0m應要求\u001b[36m日本_NORP \u001b[0m水產品、茶葉類、肉品、嬰兒食品、乳製品及加工食品等達一定數量以上，在輸入時應逐批檢附\u001b[36m日本_NORP \u001b[0m官方出具之輻射檢測報告及產地證明」。\n",
      "\u001b[33m2015年3月_DATE \u001b[0m進口爆發標示不實 雙證措施上路\n",
      "直到\u001b[33m2015年_DATE \u001b[0m，包含\u001b[36m加拿大_GPE \u001b[0m、\u001b[36m紐西蘭_GPE \u001b[0m在內的\u001b[36m13_CARDINAL \u001b[0m個國家皆逐步解除對\u001b[36m日本_GPE \u001b[0m食品輸入的管制；而\u001b[36m美國_GPE \u001b[0m與\u001b[32m歐盟_ORG \u001b[0m等國則規定包含\u001b[36m福島_GPE \u001b[0m\u001b[36m5_CARDINAL \u001b[0m縣在內，只要檢附產地證明與輻射檢測報告即可進口，當時\u001b[32m衛福部_ORG \u001b[0m部長\u001b[31m蔣丙煌_PERSON \u001b[0m表示，\u001b[36m日本_GPE \u001b[0m一直希望\u001b[36m台灣_GPE \u001b[0m能夠更加開放\u001b[36m日本_NORP \u001b[0m食品輸台，否則就必須提出科學證據。然而，\u001b[33m2015年3月_DATE \u001b[0m\u001b[36m台灣_GPE \u001b[0m爆發有廠商疑似申報不實、擅自改標，違規輸入\u001b[36m福島_GPE \u001b[0m\u001b[36m5_CARDINAL \u001b[0m縣的食品，再度引起消費者恐慌與不滿，雖然這些食品並未檢出輻射值，但政府也重新公告「\u001b[33m2015年5月15日_DATE \u001b[0m起自\u001b[36m日本_GPE \u001b[0m\u001b[36m福島_GPE \u001b[0m\u001b[36m5_CARDINAL \u001b[0m縣以外輸入食品，應檢附產地證明，特定地區之特定食品應再附上輻射檢測證明」，也就是現在就實施的雙證措施。\n",
      "\u001b[33m2015年7月_DATE \u001b[0m傳\u001b[36m日本_GPE \u001b[0m將控告\u001b[36m韓國_GPE \u001b[0m \u001b[36m台灣_GPE \u001b[0m再次考量部分開放\n",
      "\u001b[33m2015年7月下旬_DATE \u001b[0m，再度有報導指出\u001b[36m台灣_GPE \u001b[0m與\u001b[36m日本_GPE \u001b[0m已達成共識，將在\u001b[33m7月底_DATE \u001b[0m解除對\u001b[36m福島_GPE \u001b[0m以外\u001b[36m4_CARDINAL \u001b[0m個縣（\u001b[36m茨城縣_GPE \u001b[0m、\u001b[36m櫪木縣_GPE \u001b[0m、\u001b[36m群馬縣_GPE \u001b[0m、\u001b[36m千葉縣_GPE \u001b[0m），生鮮農產品與加工食品的輸\u001b[36m台_LOC \u001b[0m管制，加上當時國際盛傳\u001b[36m日本_GPE \u001b[0m將於\u001b[33m8月底_DATE \u001b[0m透過\u001b[32m世界貿易組織_ORG \u001b[0m向\u001b[36m韓國_GPE \u001b[0m提告，希望\u001b[36m韓國_GPE \u001b[0m解禁對於\u001b[36m福島_GPE \u001b[0m\u001b[36m8_CARDINAL \u001b[0m縣水產品的輸入管制。相關單位雖出面否認有受到來自\u001b[36m日本_GPE \u001b[0m的壓力，但當時\u001b[32m食藥署_ORG \u001b[0m副署長\u001b[31m吳秀英_PERSON \u001b[0m承認，的確有在考慮對於\u001b[36m福島_GPE \u001b[0m以外\u001b[36m4_CARDINAL \u001b[0m縣如加工食品的低風險食品放寬標準，但保證嬰幼兒食品不在開放的考量中。\n",
      "\u001b[33m2016年1月_DATE \u001b[0m參考國際數據 公告更嚴格的容許量標準\n",
      "過去\u001b[36m台灣_GPE \u001b[0m針對「食品中原子塵或放射能污染安全容許量標準」規定碘-\u001b[36m131_CARDINAL \u001b[0m為\u001b[36m300_CARDINAL \u001b[0m Bq/kg、銫-\u001b[36m134_CARDINAL \u001b[0m與銫-137總和為370 bq/kg，此標準與\u001b[32m國際食品法典委員會_ORG \u001b[0m（\u001b[32mCodex_ORG \u001b[0m）、\u001b[36m美國_GPE \u001b[0m與\u001b[32m歐盟_ORG \u001b[0m皆相同。而\u001b[36m日本_GPE \u001b[0m在\u001b[33m2012年4月1日_DATE \u001b[0m起調整銫-\u001b[36m134_CARDINAL \u001b[0m與銫-137的容許量標準，規定一般食品為\u001b[36m100_CARDINAL \u001b[0m Bq/kg、乳品及嬰兒食品為\u001b[34m50 Bq/kg_PRODUCT \u001b[0m、飲料及水為\u001b[36m10_CARDINAL \u001b[0m Bq/kg，我國也立即參照\u001b[36m日本_GPE \u001b[0m新修訂的標準，規定自\u001b[36m日本_GPE \u001b[0m輸入\u001b[36m台灣_GPE \u001b[0m之食品必須同時符合\u001b[36m台_GPE \u001b[0m\u001b[36m日_GPE \u001b[0m雙方的容許量標準。隨後也邀請多位專家學者參與討論，並於\u001b[33m2016年1月18日_DATE \u001b[0m修正公布。\n",
      "為了更瞭解\u001b[36m日本_GPE \u001b[0m對於核災地區之食品管控情況，\u001b[33m2016年8月21日～29日_DATE \u001b[0m由\u001b[32m外交部_ORG \u001b[0m、\u001b[32m經濟部_ORG \u001b[0m、\u001b[32m衛生福利部_ORG \u001b[0m、\u001b[32m農委會_ORG \u001b[0m、\u001b[32m原子能委員會_ORG \u001b[0m以及\u001b[36m2_CARDINAL \u001b[0m位專家共同組成跨部會團體赴\u001b[36m日_GPE \u001b[0m實地考察，考察地點包含\u001b[36m福島_GPE \u001b[0m等\u001b[36m5_CARDINAL \u001b[0m縣及\u001b[36m東京都_GPE \u001b[0m、\u001b[36m埼玉縣_GPE \u001b[0m，實地查訪農場、水產品生產場所、食品加工廠、農協蔬果選別場、輻射檢測實驗室及\u001b[32m福島第一核電廠_ORG \u001b[0m等單位。\n",
      "\u001b[33m2016年11月_DATE \u001b[0m考量分\u001b[36m兩_CARDINAL \u001b[0m階段開放 評估機制惹爭議\n",
      "直到\u001b[33m2016年11月8日_DATE \u001b[0m\u001b[36m第4_ORDINAL \u001b[0m次食安溝通與資訊交流會議時，由\u001b[32m農委會_ORG \u001b[0m副主委\u001b[31m陳吉仲_PERSON \u001b[0m再度向在場的公民團體進行「\u001b[36m日本_NORP \u001b[0m食品輸入管制措施」調整說明、\u001b[36m日本_NORP \u001b[0m食品輸台之風險分析報告，讓相關議題再度成為關注的焦點。根據此次報告表示，政府相關部會經過審慎評估後，將由「地區食品管制」改成「風險食品管制」，並分為\u001b[36m兩_CARDINAL \u001b[0m階段實施：\n",
      "\u001b[36m第一_ORDINAL \u001b[0m階段（調整實施起\u001b[33m6個月_DATE \u001b[0m內）：\n",
      "1、\u001b[36m日本_GPE \u001b[0m國內限制流通之食品亦不得輸入。\n",
      "2、\u001b[36m日本_GPE \u001b[0m野生菇類、蔬菜及水果與鳥獸肉類不得輸入。\n",
      "3、\u001b[36m日本福島縣_GPE \u001b[0m食品仍維持不得輸入之措施。\n",
      "4、\u001b[36m日本_GPE \u001b[0m\u001b[36m茨城縣_GPE \u001b[0m、\u001b[36m櫪木縣_GPE \u001b[0m、\u001b[36m群馬縣_GPE \u001b[0m、\u001b[36m千葉縣_GPE \u001b[0m等\u001b[36m4_CARDINAL \u001b[0m縣食品原則開放，唯飲用水、嬰幼兒奶粉、茶類及野生水產品等\u001b[36m4_CARDINAL \u001b[0m類食品仍維持暫停受理輸入查驗之管制措施，但輸入時須檢附\u001b[36m日本_NORP \u001b[0m官方或其授權機關開立之輻射檢測報告及產地證明，即雙證措施，\u001b[32m衛福部食藥署_ORG \u001b[0m亦須加強查驗，倘若檢測值超標，除了將該批貨物銷毀或退運，還將暫停受理該縣該項產品之輸入查驗申請。\n",
      "5、\u001b[36m日本_GPE \u001b[0m其他地區（\u001b[36m42_CARDINAL \u001b[0m都縣）食品維持開放，但須檢附\u001b[36m日本_NORP \u001b[0m官方或其授權機關開立之產地證明（單證）。\n",
      "\u001b[36m第二_ORDINAL \u001b[0m階段：\n",
      "在\u001b[36m第一_ORDINAL \u001b[0m階段實施6個月後，政府相關部門將進行檢討與評估，作為後續管制措施調整之參考。\n",
      "此外，\u001b[31m陳吉仲_PERSON \u001b[0m也表示\u001b[36m台灣_GPE \u001b[0m將會與\u001b[36m日本_GPE \u001b[0m簽署「\u001b[36m台日食品安全及進出口合作備忘錄_LAW \u001b[0m」，加強雙邊的食品安全合作，並承諾會持續針對輸入\u001b[36m台灣_GPE \u001b[0m的\u001b[36m日本_GPE \u001b[0m食品以及\u001b[36m台灣_GPE \u001b[0m的漁獲進行輻射檢測，將結果公佈於\u001b[32m食藥署_ORG \u001b[0m及\u001b[32m漁業署_ORG \u001b[0m的網站供查詢，並強調會召開\u001b[36m10_CARDINAL \u001b[0m場公聽會蒐集國人意見後再決議，而\u001b[32m行政院食品安全辦公室_ORG \u001b[0m主任\u001b[31m許輔_PERSON \u001b[0m也表示，會再向\u001b[32m原能會_ORG \u001b[0m瞭解現行輻射檢驗方法。\n",
      "\u001b[33m2016年11月_DATE \u001b[0m專家學者出面背書 社會風險溝通仍須加強\n",
      "\u001b[33m2016年11月22日_DATE \u001b[0m由\u001b[32m食安辦_ORG \u001b[0m再度召開「\u001b[36m日本特定地區食品解禁輸台及輻射交流座談會_EVENT \u001b[0m」，並找來\u001b[32m衛福部_ORG \u001b[0m、\u001b[32m農委會_ORG \u001b[0m與\u001b[32m原能會_ORG \u001b[0m向大眾說明\u001b[36m日本_GPE \u001b[0m輸入食品輻射邊境查驗資訊公開方式與後續作為、食品中原子塵或放射能污染檢驗方法以及容許量標準與安全性。而針對\u001b[32m國民黨_ORG \u001b[0m立委\u001b[31m蔣萬安_PERSON \u001b[0m提出質疑，認為\u001b[36m10_CARDINAL \u001b[0m場公聽會辦得又急又快，而且公聽會結束後依舊有高達\u001b[36m7成5_PERCENT \u001b[0m的民眾反對，食安辦主任\u001b[31m許輔_PERSON \u001b[0m坦承目前溝通仍不足，會再舉辦公聽會廣納建議，並加強與民眾的風險溝通。\n",
      "不過，根據\u001b[32m衛福部_ORG \u001b[0m、\u001b[32m原能會_ORG \u001b[0m與\u001b[32m成功大學環境微量毒物研究中心_ORG \u001b[0m主任祕書\u001b[31m李俊璋_PERSON \u001b[0m表示，目前\u001b[32m國際放射防護委員會_ORG \u001b[0m（\u001b[32mICRP_ORG \u001b[0m）制定年容許量為\u001b[36m1毫西弗_QUANTITY \u001b[0m，是考量所有慢性病患者、孕婦、老人與小孩的代謝狀況，根據再根據國人日常飲食量去推算出的輻射容許量標準，因此民眾應可以不必過於擔憂。\n",
      "\u001b[33m2016年12月_DATE \u001b[0m再度召開公聽會 廣納國人建議\n",
      "\u001b[36m2016年11月25日下午_TIME \u001b[0m，\u001b[32m行政院_ORG \u001b[0m召開「日本非福島食品進口公聽會場次記者會」，由\u001b[32m行政院_ORG \u001b[0m發言人\u001b[31m徐國勇_PERSON \u001b[0m、\u001b[32m行政院農業委員會_ORG \u001b[0m副主委\u001b[31m陳吉仲_PERSON \u001b[0m、\u001b[32m行政院食品安全辦公室_ORG \u001b[0m主任\u001b[31m許輔_PERSON \u001b[0m與\u001b[32m衛生福利部_ORG \u001b[0m次長\u001b[31m何啟功_PERSON \u001b[0m共同宣布，將於\u001b[33m12月21日_DATE \u001b[0m於\u001b[32m高雄市國立科學工藝博物館_ORG \u001b[0m、\u001b[33m12月22日_DATE \u001b[0m於\u001b[36m台北市_GPE \u001b[0m台大醫院國際會議中心301會議室舉辦公聽會，此外\u001b[36m12月25日下午_TIME \u001b[0m也將有一場電視公聽會，並開放民眾Call in，此\u001b[36m三_CARDINAL \u001b[0m場公聽會都將透過網路全程轉播。\n",
      "何啟功表示，除了這\u001b[36m3_CARDINAL \u001b[0m場公聽會，也會在食藥署官網的「\u001b[32m日本食品管理工作專區_ORG \u001b[0m」開放民眾填寫疑問與建議，廣納各界聲音，作為政策的調整方向參考。許輔也說明，這\u001b[36m3_CARDINAL \u001b[0m場公聽會依照之前公民團體的建議，不採用政府官員主持，而是邀請\u001b[32m台灣動物社會研究會_ORG \u001b[0m執行長\u001b[31m朱增宏_PERSON \u001b[0m擔任公正\u001b[36m第三_ORDINAL \u001b[0m方的主持人，期許能與關心相關議題的民眾做更深入的討論與風險溝通。\n",
      "\u001b[33m2016年12月_DATE \u001b[0m\u001b[31m吉野家納豆_PERSON \u001b[0m遭檢舉 公聽會毫無共識宣布無效\n",
      "\u001b[33m2016年12月11日_DATE \u001b[0m\u001b[36m日式_NORP \u001b[0m連鎖餐飲業者\u001b[32m吉野家_ORG \u001b[0m遭民眾檢舉，指出\u001b[32m吉野家_ORG \u001b[0m所販售的\u001b[34m納豆_PRODUCT \u001b[0m產品「\u001b[34m四付紅納豆_PRODUCT \u001b[0m」附贈的醬油包產地標示為\u001b[36m日本茨城縣_GPE \u001b[0m，然而根據目前的法規，\u001b[36m福島_GPE \u001b[0m\u001b[36m5_CARDINAL \u001b[0m縣的食品仍禁止輸入\u001b[36m台灣_GPE \u001b[0m，對此\u001b[32m食藥署_ORG \u001b[0m表示，以往在邊境抽驗\u001b[36m日本_NORP \u001b[0m食品時，主要是查驗最小販售單位（例如\u001b[36m一_CARDINAL \u001b[0m碗泡麵、\u001b[36m一_CARDINAL \u001b[0m盒納豆）產品外觀的產地標示，並未針對複合式包裝食品的內容物逐一檢查產地標示，未來將加強邊境管理，針對從\u001b[36m日本_GPE \u001b[0m進口的包裝產品逐批查核其內外包裝的產品標示與產地證明。\n",
      "此外食藥署也啟動後市場專案稽查，在\u001b[33m12月12～15日_DATE \u001b[0m結合各縣市衛生局到轄區內的賣場、超市，完成複合式包裝食品內容物的產地標示查核，同時呼籲業者應該自主清查，若有疑似來自\u001b[36m福島_GPE \u001b[0m\u001b[36m5_CARDINAL \u001b[0m縣需立即啟動預防性下架，通報各縣市主管機關，在未釐清產品產地之前需暫停販售。\n",
      "不過此事件也激起公民團體與民眾的憤怒，\u001b[33m2016年12月25日_DATE \u001b[0m於\u001b[36m新北市_GPE \u001b[0m舉辦的「\u001b[36m日本_GPE \u001b[0m核災後食品風險危害評估管理及\u001b[36m茨城_GPE \u001b[0m、\u001b[36m櫪木_GPE \u001b[0m、\u001b[36m千葉_GPE \u001b[0m、\u001b[36m群馬_GPE \u001b[0m食品開放與否公聽會」，不只會場外聚集\u001b[36m數百_CARDINAL \u001b[0m人抗議，會場內也有許多公民團體不滿政府提出的數據都是由\u001b[36m日本_GPE \u001b[0m提供，且政府檢驗、把關的能力也備受質疑，在公聽會過程受到嚴重干擾、無法進行討論、雙方毫無共識的結果下，公聽會主持人宣布「公聽會無效」，而預計在\u001b[33m2017年1月_DATE \u001b[0m舉辦的\u001b[36m2_CARDINAL \u001b[0m場公聽會也宣布延期。\n",
      "\u001b[33m2018年11月_DATE \u001b[0m全民公投：通過「維持禁止\u001b[36m日本_GPE \u001b[0m核災區食品進口」\n",
      "\u001b[33m2018年11月24日_DATE \u001b[0m依據《\u001b[36m公民投票法_LAW \u001b[0m》舉行之全國性公民投票，\u001b[36m十_CARDINAL \u001b[0m大公投案中針對\u001b[36m日本_GPE \u001b[0m食品「維持禁核災區食品進口」以過半數通過，當時根據《\u001b[36m食力_WORK_OF_ART \u001b[0m》統計\u001b[33m2018年_DATE \u001b[0m最受\u001b[32mFacebook_ORG \u001b[0m網友熱議的\u001b[36m十大食安事件_EVENT \u001b[0m排行榜中，「\u001b[36m日本_GPE \u001b[0m福島\u001b[36m五_CARDINAL \u001b[0m縣地區食品」進口管制議題，登上聲量排行榜\u001b[36m第2_ORDINAL \u001b[0m名，討論聲量高達\u001b[36m961_CARDINAL \u001b[0m,836，僅次於\u001b[36m非洲_GPE \u001b[0m豬瘟疫情。\n",
      "\u001b[36m日本_GPE \u001b[0m外務大臣\u001b[31m河野太郎_PERSON \u001b[0m表示「反核食公投」的通過，可能會影響到\u001b[36m台灣_GPE \u001b[0m加入跨\u001b[36m太平洋_LOC \u001b[0m夥伴全面進展協定（CPTPP）的機會。對此，\u001b[32m行政院食品安全辦公室_ORG \u001b[0m主任\u001b[31m許輔_PERSON \u001b[0m表示，\u001b[32m行政院_ORG \u001b[0m希望\u001b[36m日方_GPE \u001b[0m能尊重國人公投的決定，也希望\u001b[36m台_GPE \u001b[0m\u001b[36m日_GPE \u001b[0m外交與經貿不因此受到影響，因為食安、國貿皆應該「雙贏」才能有利\u001b[36m台灣_GPE \u001b[0m發展。\n",
      "\u001b[33m2022年2月_DATE \u001b[0m\u001b[32m行政院_ORG \u001b[0m預告 福島\u001b[36m五_CARDINAL \u001b[0m縣食品正式解禁\n",
      "\u001b[33m2022年2月8日_DATE \u001b[0m\u001b[32m行政院_ORG \u001b[0m以行政命令方式宣布「\u001b[36m日本_GPE \u001b[0m\u001b[36m福島_GPE \u001b[0m\u001b[36m五_CARDINAL \u001b[0m縣地區食品進口」正式解禁，預告期\u001b[33m10天_DATE \u001b[0m，將於\u001b[33m2022年2月中下旬_DATE \u001b[0m完成公告輸入程序。\n",
      "食藥署署長\u001b[31m吳秀梅_PERSON \u001b[0m說明，本次解禁有\u001b[36m三_CARDINAL \u001b[0m大原則把關：\n",
      "1、\u001b[36m福島_GPE \u001b[0m\u001b[36m五_CARDINAL \u001b[0m縣食品現行規範將從「禁止特定地區進口」改為「禁止特定品項進口」。\n",
      "2、針對高風險品項需提供「雙證」，提供輻射檢驗證明、產地證明。\n",
      "3、所有產品在邊境需逐批檢驗合格才能放行。\n",
      "\u001b[32m衛福部_ORG \u001b[0m部長\u001b[31m陳時中_PERSON \u001b[0m表示，\u001b[33m2011年_DATE \u001b[0m核污發生時，出自於對於安全風險無法有效的掌握，當時選擇以區域的方式進行管制。至今\u001b[33m11年_DATE \u001b[0m過去，期間透過不斷嚴謹的審查安全程度，以科學證據證明安全無虞的情勢下決定解禁。但不管是出於對加入跨\u001b[36m太平洋_LOC \u001b[0m夥伴全面進展協定（CPTPP）的考量，或是希望邊境管制、檢驗方法能跟進國際腳步，回頭看著\u001b[33m2016年_DATE \u001b[0m的公聽會系列操作與\u001b[33m2018年_DATE \u001b[0m公投結果，這一看似不顧民意的行政命令頒佈，無疑是挑戰民眾對於政府的信任度。\n",
      "審稿編輯：童儀展、林玉婷\n",
      "延伸閱讀\n",
      "▶研究證實：就算\u001b[33m全年_DATE \u001b[0m吃\u001b[36m福島_GPE \u001b[0m\u001b[36m五_CARDINAL \u001b[0m縣食品 輻射量也低於一趟長程班機\n",
      "▶網傳\u001b[36m日本_NORP \u001b[0m核災食品有特殊代碼？\u001b[32m食藥署_ORG \u001b[0m：不實謠言\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random \n",
    "news_id = random.randint(0, len(df))\n",
    "print('==== ckip tagger ===')\n",
    "print(news_id)\n",
    "text = df.iloc[news_id][textcol] \n",
    "nertokens = df.iloc[news_id]['ner_ckiptagger_text']\n",
    "color_ner_text(text = text, nertokens = nertokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['ner_text', 'ner_title'])\n",
    "df.to_pickle(os.path.join(data_dir, filename))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaddleNLP: UIE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import opencc\n",
    "def t2sconvert(texts: Union[List, Dict, str])-> Union[List, Dict, str]:\n",
    "\n",
    "    converter = opencc.OpenCC('t2s.json')\n",
    "    if isinstance(texts, str):\n",
    "        new_texts = converter.convert(texts)\n",
    "    elif isinstance(texts, list):\n",
    "        new_texts = []\n",
    "        for i in range(len(texts)):\n",
    "            new_texts.append(converter.convert(texts[i]))\n",
    "    elif isinstance(texts, dict):\n",
    "        new_texts = {}\n",
    "        for key, value in texts.items():\n",
    "            key = converter.convert(key)\n",
    "            new_texts[key] = t2sconvert(value)\n",
    "    assert len(texts) == len(new_texts)\n",
    "    del texts\n",
    "    return new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\"機構\": [], \"化學物質\": [\"治癌物\", \"農藥\"], \"產品\": [], \"時間\": []} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from easydict import EasyDict as edict\n",
    "keymap = {\n",
    "    '機構': 'ORG',\n",
    "    '化學物質': 'CHEMICAL',\n",
    "    '產品': 'PRODUCT',\n",
    "    '時間': 'TIME',\n",
    "}\n",
    "def uie2nertokens(uie: Dict)-> List:\n",
    "    \"\"\" Convert uie output to a list of NER tokens. \n",
    "        Args:\n",
    "            uie (Dict): uie output, a dict with\n",
    "                the schema keys and a list of tuples as extracted tokens. \n",
    "            Examples: \n",
    "            {'产品': [{'end': 83,\n",
    "                    'probability': 0.32381882254490435,\n",
    "                    'start': 79,\n",
    "                    'text': '三苯醋锡'},\n",
    "                    {'end': 61,\n",
    "                    'probability': 0.4285592181817037,\n",
    "                    'start': 56,\n",
    "                    'text': '氯化三苯锡'}], \n",
    "            '机构': [...]} \n",
    "        Returns:\n",
    "            List: a list of ner tokens simulated with edict()  \n",
    "    \"\"\"\n",
    "    nertoks = [] \n",
    "    for k in schema:\n",
    "        simpk = t2sconvert(k)\n",
    "        v = uie.get(simpk, []) \n",
    "        entity_name = keymap.get(k, k)\n",
    "        for entity in v: \n",
    "            nertok = edict()\n",
    "            nertok.idx = (entity['start'], entity['end'])\n",
    "            nertok.ner = entity_name\n",
    "            nertoks.append(nertok)\n",
    "    return nertoks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# colormap = {\n",
    "#     'PERSON': 'red',\n",
    "#     'FOOD': 'blue',\n",
    "#     'PRODUCT': 'blue',\n",
    "#     'CHEMICAL': 'magenta',      \n",
    "#     'ORG': 'green',\n",
    "#     'DATE': 'yellow', \n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "# news_id = random.randint(0, len(df))\n",
    "# print(news_id)\n",
    "news_id  = 13 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▲\u001b[32m哈根達斯_ORG \u001b[0m香草冰淇淋再被驗出致癌物\u001b[35m環氧乙烷_CHEMICAL \u001b[0m。（圖／\u001b[32m食藥署_ORG \u001b[0m提供）\n",
      "記者鄒鎮宇／綜合報導\n",
      "美國知名冰淇淋品牌「Häagen-Dazs（哈根達斯）」日前進口台灣的「\u001b[34m香草冰淇淋」_PRODUCT \u001b[0m驗出禁用農藥，被邊境攔下依規定銷毀。沒想到，\u001b[32m香港食物環境衞生署食物安全中心_ORG \u001b[0m10日發出公告，\u001b[32m哈根達斯_ORG \u001b[0m的香草冰淇淋被驗出\u001b[32m歐盟_ORG \u001b[0m禁用的農藥\u001b[35m環氧乙烷_CHEMICAL \u001b[0m，因此要求業界停止使用或出售。\n",
      "據《東網》報導，\u001b[32m香港食物環境衞生署食物安全中心_ORG \u001b[0m發現，\u001b[32m哈根達斯_ORG \u001b[0m75毫升、100毫升、473毫升、9.46公升裝的香草冰淇淋被驗出農藥\u001b[35m環氧乙烷_CHEMICAL \u001b[0m，因此立刻跟進口商溝通，並通知業界停止使用或出售，後續將進行調查。\n",
      "\u001b[32m哈根達斯_ORG \u001b[0m6月21日時也在香港被驗出有\u001b[35m環氧乙烷_CHEMICAL \u001b[0m，當時香港\u001b[32m哈根達斯_ORG \u001b[0m致歉，並停售、撤回商品，豈料本月10日又再被驗出含有\u001b[35m環氧乙烷_CHEMICAL \u001b[0m。\n",
      "據悉，\u001b[32m哈根達斯_ORG \u001b[0m6月21日時進口台灣的香草冰淇淋也驗出環氧乙烷，在邊境被攔下1164盒、5471.34公斤的產品，因不符合食品安全衛生管理法第15條有關「農藥殘留容許量標準」規定，須依規定退運或銷毀。\n",
      "\u001b[32m食藥署北區管理中心_ORG \u001b[0m簡任技正吳宗熹過去受訪時指出，環氧乙烷為農藥一種，具致癌風險，依目前規定不在食品中檢出，國內也沒有核准作為農藥使用，因此進口產品也不得檢出。\n",
      "►按這訂閱Podcast《小編沒收工》每天熱門話題聊不完\n"
     ]
    }
   ],
   "source": [
    "filename = '食品安全_uie.csvpkl'\n",
    "df = pd.read_pickle(os.path.join(data_dir, filename)) \n",
    "text = df.iloc[news_id][textcol] \n",
    "uie_output = df.iloc[news_id]['uie_base'][0]\n",
    "nertokens = uie2nertokens(uie_output)\n",
    "color_ner_text(text = text, nertokens = nertokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Droidtown:Articut \n",
    "\n",
    "#### References \n",
    "https://www.jsjkx.com/EN/10.11896/jsjkx.200800181 -> food safety news \n",
    "#### Issue: chemical substance: ckip-transformers 環氧乙烷 抓不出來 (Chemical NER) \n",
    "-> 求助易庭的 dictionary tree  \n",
    "-> 卓騰 Articut Chemical NER (Python API available): https://blog.droidtown.co/post/643573663484067840/chemical \n",
    "\n",
    "\n",
    "#### Issue: ckip-transformers 香草冰淇淋抓不出來（food NER)  \n",
    "-> 卓騰 Articut Food NER: https://api.droidtown.co/ArticutAPI/document/#ArticutAPI\n",
    " \n",
    "- foodBERT\n",
    "  https://github.com/chambliss/foodbert -> English only  \n",
    "- Chinese ner dataset: https://zhuanlan.zhihu.com/p/529541521\n",
    "    - ner food related dataset: 萬創杯中醫相關命名實體辨識資料集\n",
    "      https://aiqianji.com/openoker/Chinese-DeepNER-Pytorch \n",
    "    - 其中有食物的 entity annotation，但是感覺比較偏向原型食物：\n",
    "```\n",
    "食物(FOOD):指能够满足机体正常生理和生化能量需求，并能延续正常寿命的物质。对人体而言，能够满足人的正常生活活动需求并利于寿命延长的物质称之为食物。例子：苹果、茶、木耳、萝卜\n",
    "食物分组(FOOD_GROUP): 中医中饮食养生中，将食物分为寒热温凉四性，同时中医药禁忌中对于具有某类共同属性食物的统称，记为食物分组。例子：油腻食物、辛辣食物、凉性食物\n",
    "```\n",
    "- English food dataset: \n",
    "- TASTEset (Recipe Dataset and Food Entities Recoginition Benchmark): https://github.com/taisti/tasteset \n",
    "```\n",
    "ingredients,ingredients_entities\n",
    "\"5 ounces rum\n",
    "4 ounces triple sec\n",
    "3 ounces Tia Maria\n",
    "20 ounces orange juice\n",
    "\",\"[{\"\"start\"\": 0, \"\"end\"\": 1, \"\"type\"\": \"\"QUANTITY\"\", \"\"entity\"\": \"\"5\"\"},{\"\"start\"\": 2, \"\"end\"\": 8, \"\"type\"\": \"\"UNIT\"\", \"\"entity\"\": \"\"ounces\"\"},{\"\"start\"\": 9, \"\"end\"\": 12, \"\"type\"\": \"\"FOOD\"\", \"\"entity\"\": \"\"rum\"\"},{\"\"start\"\": 13, \"\"end\"\": 14, \"\"type\"\": \"\"QUANTITY\"\", \"\"entity\"\": \"\"4\"\"},{\"\"start\"\": 15, \"\"end\"\": 21, \"\"type\"\": \"\"UNIT\"\", \"\"entity\"\": \"\"ounces\"\"},{\"\"start\"\": 22, \"\"end\"\": 32, \"\"type\"\": \"\"FOOD\"\", \"\"entity\"\": \"\"triple sec\"\"},{\"\"start\"\": 33, \"\"end\"\": 34, \"\"type\"\": \"\"QUANTITY\"\", \"\"entity\"\": \"\"3\"\"},{\"\"start\"\": 35, \"\"end\"\": 41, \"\"type\"\": \"\"UNIT\"\", \"\"entity\"\": \"\"ounces\"\"},{\"\"start\"\": 42, \"\"end\"\": 51, \"\"type\"\": \"\"FOOD\"\", \"\"entity\"\": \"\"Tia Maria\"\"},{\"\"start\"\": 52, \"\"end\"\": 54, \"\"type\"\": \"\"QUANTITY\"\", \"\"entity\"\": \"\"20\"\"},{\"\"start\"\": 55, \"\"end\"\": 61, \"\"type\"\": \"\"UNIT\"\", \"\"entity\"\": \"\"ounces\"\"},{\"\"start\"\": 62, \"\"end\"\": 74, \"\"type\"\": \"\"FOOD\"\", \"\"entity\"\": \"\"orange juice\"\"}]\"\n",
    "``` \n",
    "- FoodBase corpus \n",
    "https://academic.oup.com/database/article/doi/10.1093/database/baz121/5611291?login=true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ArticutAPI import Articut\n",
    "\n",
    "articut = Articut() \n",
    "# default: public quota mode \n",
    "# 每小時更新 2000 字\n",
    "news_id = 13 \n",
    "inputSTR = df.iloc[news_id][textcol]\n",
    "\n",
    "result = articut.parse(inputSTR, chemicalBOOL = True)\n",
    "# food_ner_tokens = articut.NER.getFood(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict \n",
    "\n",
    "def format_ner_tokens(ner_tokens, type = 'FOOD'):\n",
    "    raw = [edict({'idx': (t[0][1] - len(t[0][2]), t[0][1]), \n",
    "                   'text': t[0][2],\n",
    "                   'ner': type}) for t in ner_tokens if len(t) > 0]\n",
    "    return raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chemical_ner_tokens = articut.getChemicalLIST(result, indexWithPOS=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemical_ner_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get correct indices (navigating into Droidtown's code)\n",
    "\"\"\" # ARTICUTAPI/ToolKit/toolkits.py\n",
    "    def getChemicalLIST(self, parseResultDICT, indexWithPOS=True):\n",
    "        '''\n",
    "        取出斷詞結果中的 KNOWLEDGE_chemical。\n",
    "        每個句子內的 KNOWLEDGE_chemical 為一個 list.\n",
    "        '''\n",
    "        if \"result_pos\" in parseResultDICT:\n",
    "            pass\n",
    "        else:\n",
    "            return None\n",
    "        chemicalLIST = []\n",
    "        for p in parseResultDICT[\"result_pos\"]:\n",
    "            if len(p) > 1:\n",
    "                chemicalLIST.append([(c.start(), c.end(), c.group(0))\n",
    "                                     for c in list(self.chemicalPat.finditer(p))])\n",
    "            else:\n",
    "                chemicalLIST.append([])\n",
    "        if not indexWithPOS:\n",
    "            chemicalLIST = self._segIndexConverter(parseResultDICT, chemicalLIST)\n",
    "        return chemicalLIST\n",
    "\"\"\"\n",
    "nertype = 'CHEMICAL'\n",
    "\n",
    "result_pos = result['result_pos']\n",
    "paragraph = ''\n",
    "for p, t in zip(result_pos, chemical_ner_tokens):\n",
    "    if len(t) > 0:\n",
    "        t.sort(key = lambda x: x[-1], reverse = True)\n",
    "        for (b, e, token_text) in t:\n",
    "            typed_text = p[b:e] + f'_{nertype}' + ' '\n",
    "            color = colormap.get(nertype, 'cyan')\n",
    "            span = colored(typed_text, color) \n",
    "            p = p[:b] + span + p[e:]\n",
    "            # print(p)\n",
    "            paragraph += p\n",
    "    else: \n",
    "        paragraph += p if isinstance(p, str) else ' '.join(p)\n",
    "# print(paragraph)\n",
    "# format_chemi_tokens = format_ner_tokens(chemical_ner_tokens , type = 'CHEMICAL')\n",
    "# format_chemi_tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▲哈根達斯香草冰淇淋再被驗出致癌物\u001b[35m環氧乙烷_CHEMICAL \u001b[0m。（圖／食藥署提供）\n",
      "記者鄒鎮宇／綜合報導\n",
      "美國知名冰淇淋品牌「Häagen-Dazs（哈根達斯）」日前進口台灣的「香草冰淇淋」驗出禁用農藥，被邊境攔下依規定銷毀。沒想到，香港食物環境衞生署食物安全中心10日發出公告，哈根達斯的香草冰淇淋被驗出歐盟禁用的農藥\u001b[35m環氧乙烷_CHEMICAL \u001b[0m，因此要求業界停止使用或出售。\n",
      "據《東網》報導，香港食物環境衞生署食物安全中心發現，哈根達斯75毫升、100毫升、473毫升、9.46公升裝的香草冰淇淋被驗出農藥\u001b[35m環氧乙烷_CHEMICAL \u001b[0m，因此立刻跟進口商溝通，並通知業界停止使用或出售，後續將進行調查。\n",
      "哈根達斯6月21日時也在香港被驗出有\u001b[35m環氧乙烷_CHEMICAL \u001b[0m，當時香港哈根達斯致歉，並停售、撤回商品，豈料本月10日又再被驗出含有\u001b[35m環氧乙烷_CHEMICAL \u001b[0m。\n",
      "據悉，哈根達斯6月21日時進口台灣的香草冰淇淋也驗出\u001b[35m環氧乙烷_CHEMICAL \u001b[0m，在邊境被攔下1164盒、5471.34公斤的產品，因不符合食品安全衛生管理法第15條有關「農藥殘留容許量標準」規定，須依規定退運或銷毀。\n",
      "食藥署北區管理中心簡任技正吳宗熹過去受訪時指出，\u001b[35m環氧乙烷_CHEMICAL \u001b[0m為農藥一種，具致癌風險，依目前規定不在食品中檢出，國內也沒有核准作為農藥使用，因此進口產品也不得檢出。\n",
      "►按這訂閱Podcast《小編沒收工》每天熱門話題聊不完\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  clean all the <tag></tag> \n",
    "import re \n",
    "CLEANR = re.compile('<.*?>') \n",
    "\n",
    "def cleantags(raw_html):\n",
    "  cleantext = re.sub(CLEANR, '', raw_html)\n",
    "  return cleantext\n",
    "\n",
    "clean_paragraph = cleantags(paragraph)\n",
    "print(clean_paragraph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d36a7b1d5b86f765154479f5ba9c0cd4edaa9a5fcd483bcf496f71249c97c50f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
